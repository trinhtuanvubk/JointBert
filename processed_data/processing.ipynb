{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangdt/research/JointBert/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'modeling_roberta' from 'transformers' (/home/sangdt/research/JointBert/venv/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m modeling_roberta\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'modeling_roberta' from 'transformers' (/home/sangdt/research/JointBert/venv/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import modeling_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open(\"/home/sangdt/research/intent-entity-bert/entity_recognitor/data/test_slots.json\") as f:\n",
    "    for line in f:\n",
    "        data_test.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sangdt/research/JointBert/data/test_slots.jsonl', 'w') as outfile:\n",
    "    for entry in data_test:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 7747, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"alo\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_intents = ['activation', 'agent', 'ask_info', 'barring', 'cancellation', 'change',\n",
    " 'connection', 'continue', 'downloading', 'exit', 'greetings', 'help', 'leaving',\n",
    " 'no_intention', 'ordering', 'payment', 'purchase', 'refunding' ,'register',\n",
    " 'renewal' ,'repairing' ,'repeat' ,'report_issues', 'returning' ,'start',\n",
    " 'top-up', 'transferring', 'unblocking', 'unlocking', 'upgrading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distinct_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'custom', 'er', 'Ġservice', '</s>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"<s>customer service</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_slot(list_slots, sentence):\n",
    "    list_bert_tokens = tokenizer.tokenize(sentence.strip())\n",
    "    attention_mask = tokenizer(sentence.strip(), add_special_tokens=True)['attention_mask']\n",
    "    list_index_g = [index for index, i in enumerate(list_bert_tokens) if 'Ġ' in i]\n",
    "    list_index_g = [0] + list_index_g\n",
    "    new_list_slots = []\n",
    "    for index, token in enumerate(list_bert_tokens):\n",
    "        if index not in list_index_g:\n",
    "            tmp = [i for i in list_index_g if i < index][-1]\n",
    "            ner_tag = list_slots[list_index_g.index(tmp)].replace(\"B-\", \"I-\")\n",
    "        else:\n",
    "            ner_tag = list_slots[list_index_g.index(index)]\n",
    "        new_list_slots.append(ner_tag)\n",
    "    list_ids_tokens = [vocab[i] for i in list_bert_tokens]\n",
    "    return {\"token_ids\": [0] + list_ids_tokens + [2], \"labels\": ['O'] +  new_list_slots + ['O'], \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sentences, list_slots):\n",
    "    new_list_slots = []\n",
    "    list_ids_slots = []\n",
    "    list_pre = tokenizer(sentences, padding=True, truncation=True, add_special_tokens=True)\n",
    "    for i in range(len(sentences)):\n",
    "        list_tokens = tokenizer.convert_ids_to_tokens(list_pre['input_ids'][i])\n",
    "        list_index_g = [index for index, i in enumerate(list_tokens) if 'Ġ' in i]\n",
    "        list_index_g = [1] + list_index_g\n",
    "        new_slots = []\n",
    "        for index, token in enumerate(list_tokens):\n",
    "            if index not in list_index_g:\n",
    "                if token in ['<s>', '</s>', '<pad>']:\n",
    "                    ner_tag = 'O'\n",
    "                else:\n",
    "                    tmp = [i for i in list_index_g if i < index][-1]\n",
    "                    ner_tag = list_slots[i][list_index_g.index(tmp)].replace(\"B-\", \"I-\")\n",
    "            else:\n",
    "                ner_tag = list_slots[i][list_index_g.index(index)]\n",
    "            new_slots.append(ner_tag)\n",
    "        new_list_slots.append(new_slots)\n",
    "\n",
    "    for slots in new_list_slots:\n",
    "        list_ids_slots.append([dict_tags[i] for i in slots])\n",
    "    return (\n",
    "        torch.tensor(list_pre['input_ids']),\n",
    "        torch.tensor(list_pre['attention_mask']),\n",
    "        torch.tensor(list_ids_slots)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [1,2,3]], dtype=   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sentences = [example['text'] for example in batch]\n",
    "    list_slots = [example['slots'] for example in batch]\n",
    "    return transform(sentences, list_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = list(set([example for i in distinct_slots for example in [i, i.replace(\"B-\", \"I-\")]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tags = {tag: i for i, tag in enumerate(new_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tags['B-CUSTOMER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sangdt/research/JointBert/processed_data/intent_label.txt', 'w') as f:\n",
    "    for line in distinct_intents:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_train = []\n",
    "for example in data_train:\n",
    "    new_data_train.append(modify_slot(example['slots'], example['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4042"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'customer services please', 'tokens': ['customer', 'services', 'please'], 'intention': 'agent', 'slots': ['B-CUSTOMER', 'B-SERVICE', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(data_train[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': [0, 31458, 254, 518, 2540, 2], 'labels': ['O', 'B-CUSTOMER', 'I-CUSTOMER', 'B-SERVICE', 'O', 'O'], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(new_data_train[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'Ġenc', 'oder', 'bi', 'Ġenc', 'oder']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(('a encoder', 'bi encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'custom', 'er', 'Ġservices', 'Ġplease', '</s>']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 31458, 254, 518, 2540, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 6621, 2, 1], [0, 14141, 2, 1], [0, 7309, 47821, 2]], 'attention_mask': [[1, 1, 1, 0], [1, 1, 1, 0], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['aa', 'bb', 'cc dd'], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data_train, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    0, 31458,   254,   544,     2,     1,     1,     1,     1],\n",
      "        [    0,   858, 15027,     2,     1,     1,     1,     1,     1],\n",
      "        [    0,  2362,  3392,    47,     2,     1,     1,     1,     1],\n",
      "        [    0,  4651,  1355,     2,     1,     1,     1,     1,     1],\n",
      "        [    0, 39397,     7,  2111,   544,     2,     1,     1,     1],\n",
      "        [    0,  2558,  2753,  1355,     2,     1,     1,     1,     1],\n",
      "        [    0,   560,  1994,     7,  2111,   544,     2,     1,     1],\n",
      "        [    0, 11131,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  2362,  2888,  2748,     2,     1,     1,     1,     1],\n",
      "        [    0, 31458,   254,   544,  2540,     2,     1,     1,     1],\n",
      "        [    0, 28825,  1052,     2,     1,     1,     1,     1,     1],\n",
      "        [    0,   560,  1067,     7,  2111,   544,     2,     1,     1],\n",
      "        [    0, 28023,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 31458,   254,   518,  2540,     2,     1,     1,     1],\n",
      "        [    0,  4082,   260,  1288,     2,     1,     1,     1,     1],\n",
      "        [    0,   118,   236,     7,  1994,     7,  2111,   544,     2]]), tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[ 84, 203,  41, 173,  84,  84,  84,  84,  84],\n",
      "        [ 84,  74, 181,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84,  74,  84,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84,  88,  92,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84,  84,  84, 203, 173,  84,  84,  84,  84],\n",
      "        [ 84,  54, 104,  92,  84,  84,  84,  84,  84],\n",
      "        [ 84,  84,  84,  84, 203, 173,  84,  84,  84],\n",
      "        [ 84, 173,  84,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84,  74,  11,  89,  84,  84,  84,  84,  84],\n",
      "        [ 84, 203,  41, 173,  84,  84,  84,  84,  84],\n",
      "        [ 84, 166,  84,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84,  84,  84,  84, 203, 173,  84,  84,  84],\n",
      "        [ 84, 173,  84,  84,  84,  84,  84,  84,  84],\n",
      "        [ 84, 203,  41, 173,  84,  84,  84,  84,  84],\n",
      "        [ 84,  73, 107,  92,  84,  84,  84,  84,  84],\n",
      "        [ 84,  84,  84,  84,  84,  84, 203, 173,  84]]))\n"
     ]
    }
   ],
   "source": [
    "for i in data_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<cls>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab[\u001b[39m'\u001b[39;49m\u001b[39m<cls>\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '<cls>'"
     ]
    }
   ],
   "source": [
    "vocab['<cls>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(tmp, [[1, 2, 3,]], axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "938cf2c0d2cf35b9762ac63a821a45f89d813e0b52d6a2f30cb1dba2a6a5f534"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
